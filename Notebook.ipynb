{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing necessary modules**\n",
        "\n",
        "\n",
        "\n",
        "*   numpy for numerical computing.\n",
        "*   to_categorical function from `keras.utils` for one-hot encoding labels.\n",
        "\n",
        "\n",
        "*   ImageDataGenerator from `tensorflow.keras.preprocessing.image` for image data preprocessing.\n",
        "*   `Sequential`, `Dropout`, `Flatten`, and `Dense` layers from `tensorflow.keras.layers` for building the neural network model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lej5Q0GPiF8A"
      },
      "id": "Lej5Q0GPiF8A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2926811d",
      "metadata": {
        "id": "2926811d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras import applications\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.src.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import utils\n",
        "utils.to_categorical\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting up Paths and Image Dimensions for Transfer Learning with Keras**"
      ],
      "metadata": {
        "id": "BMck15fKkkfA"
      },
      "id": "BMck15fKkkfA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4c34136-eeb1-4905-bcca-e1b4ea4a1682",
      "metadata": {
        "id": "e4c34136-eeb1-4905-bcca-e1b4ea4a1682"
      },
      "outputs": [],
      "source": [
        "#from tensorflow.keras.\n",
        "import math\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
        "train_data_dir = '/Users/saryuvasishat/Desktop/SOEN Project/es-crop-disease-diagnosis-master/dataset/train'\n",
        "validation_data_dir = '/Users/saryuvasishat/Desktop/SOEN Project/es-crop-disease-diagnosis-master/dataset/test'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating and saving bottleneck features using a pre-trained VGG16 model.**\n",
        "\n",
        "\n",
        "*   Loading the VGG16 model pretrained on the ImageNet dataset, excluding the top classification layers (include_top=False).\n",
        "\n",
        "*   Setting up an ImageDataGenerator to preprocess the image data.\n",
        "\n",
        "*   Creating a generator for the training data directory, setting target size, batch size, and other parameters.\n",
        "\n",
        "*   The generator is used to predict the bottleneck features for the training data using the pre-trained VGG16 model. These features are then saved to a file named 'bottleneck_features_train.npy'\n",
        "\n",
        "*   Similarly, bottleneck features are generated for the validation data and saved to a file named 'bottleneck_features_validation.npy'\n",
        "\n",
        "*   The function prints out some information about the filenames and class indices for both training and validation data directories.\n",
        "\n"
      ],
      "metadata": {
        "id": "8u88JkFg5lSc"
      },
      "id": "8u88JkFg5lSc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7e1496f-a5fe-4f3c-a821-e5ca7cbf38e3",
      "metadata": {
        "id": "e7e1496f-a5fe-4f3c-a821-e5ca7cbf38e3"
      },
      "outputs": [],
      "source": [
        "def save_bottlebeck_features():\n",
        "    # build the VGG16 network\n",
        "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
        "\n",
        "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "\n",
        "    print(len(generator.filenames))\n",
        "    print(generator.class_indices)\n",
        "    print(len(generator.class_indices))\n",
        "\n",
        "    nb_train_samples = len(generator.filenames)\n",
        "    num_classes = len(generator.class_indices)\n",
        "\n",
        "    predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
        "\n",
        "    bottleneck_features_train = model.predict(\n",
        "        generator, predict_size_train)\n",
        "\n",
        "    np.save('bottleneck_features_train.npy', bottleneck_features_train)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "\n",
        "    nb_validation_samples = len(generator.filenames)\n",
        "\n",
        "    predict_size_validation = int(\n",
        "        math.ceil(nb_validation_samples / batch_size))\n",
        "\n",
        "    bottleneck_features_validation = model.predict_generator(\n",
        "        generator, predict_size_validation)\n",
        "\n",
        "    np.save('bottleneck_features_validation.npy',\n",
        "            bottleneck_features_validation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the model**\n",
        "\n",
        "\n",
        "*   The train_top_model function starts by setting up an\n",
        "ImageDataGenerator to preprocess the image data.\n",
        "\n",
        "*   It loads bottleneck features (precomputed representations) for the training data from a file using np.load.\n",
        "\n",
        "*   The class labels for the training data are retrieved and converted into categorical labels using one-hot encoding.\n",
        "\n",
        "*   Similarly, bottleneck features and labels for validation data are loaded and processed.\n",
        "\n",
        "*   A new neural network model is defined using Keras Sequential API. It consists of a Flatten layer followed by a Dense layer with ReLU activation and a Dropout layer to prevent overfitting. The output layer is a Dense layer with softmax activation for multi-class classification.\n",
        "\n",
        "*   The model is compiled with 'rmsprop' optimizer and 'categorical_crossentropy' loss function.\n",
        "\n",
        "*   Training the model is performed using the fit method with the training data, labels, validation data, and labels. The training history is stored in the history variable.\n",
        "\n",
        "*   After training, the model weights are saved to a file.\n",
        "\n",
        "Model evaluation is done using the evaluate method on the validation data.\n",
        "\n",
        "Finally, the accuracy and loss of the model are printed.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "064f-SDz1VCn"
      },
      "id": "064f-SDz1VCn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "104a88aa-e841-4269-a664-2d77609066cb",
      "metadata": {
        "id": "104a88aa-e841-4269-a664-2d77609066cb",
        "outputId": "cc8f83d6-044f-4bd6-8feb-cfcdbe9d37a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 43456 images belonging to 38 classes.\n",
            "43456\n",
            "{'Apple___Apple_scab': 0, 'Apple___Black_rot': 1, 'Apple___Cedar_apple_rust': 2, 'Apple___healthy': 3, 'Blueberry___healthy': 4, 'Cherry_(including_sour)___Powdery_mildew': 5, 'Cherry_(including_sour)___healthy': 6, 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': 7, 'Corn_(maize)___Common_rust_': 8, 'Corn_(maize)___Northern_Leaf_Blight': 9, 'Corn_(maize)___healthy': 10, 'Grape___Black_rot': 11, 'Grape___Esca_(Black_Measles)': 12, 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)': 13, 'Grape___healthy': 14, 'Orange___Haunglongbing_(Citrus_greening)': 15, 'Peach___Bacterial_spot': 16, 'Peach___healthy': 17, 'Pepper,_bell___Bacterial_spot': 18, 'Pepper,_bell___healthy': 19, 'Potato___Early_blight': 20, 'Potato___Late_blight': 21, 'Potato___healthy': 22, 'Raspberry___healthy': 23, 'Soybean___healthy': 24, 'Squash___Powdery_mildew': 25, 'Strawberry___Leaf_scorch': 26, 'Strawberry___healthy': 27, 'Tomato___Bacterial_spot': 28, 'Tomato___Early_blight': 29, 'Tomato___Late_blight': 30, 'Tomato___Leaf_Mold': 31, 'Tomato___Septoria_leaf_spot': 32, 'Tomato___Spider_mites Two-spotted_spider_mite': 33, 'Tomato___Target_Spot': 34, 'Tomato___Tomato_Yellow_Leaf_Curl_Virus': 35, 'Tomato___Tomato_mosaic_virus': 36, 'Tomato___healthy': 37}\n",
            "38\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1902s\u001b[0m 700ms/step\n",
            "Found 10849 images belonging to 38 classes.\n",
            "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 690ms/step\n",
            "Found 43456 images belonging to 38 classes.\n",
            "Found 10849 images belonging to 38 classes.\n",
            "Epoch 1/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.5436 - loss: 1.6807 - val_accuracy: 0.8468 - val_loss: 0.4980\n",
            "Epoch 2/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7376 - loss: 0.9049 - val_accuracy: 0.8581 - val_loss: 0.4665\n",
            "Epoch 3/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7595 - loss: 0.8668 - val_accuracy: 0.8652 - val_loss: 0.4716\n",
            "Epoch 4/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7718 - loss: 0.8433 - val_accuracy: 0.8632 - val_loss: 0.4881\n",
            "Epoch 5/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7793 - loss: 0.8563 - val_accuracy: 0.8746 - val_loss: 0.4934\n",
            "Epoch 6/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7805 - loss: 0.8801 - val_accuracy: 0.8804 - val_loss: 0.4709\n",
            "Epoch 7/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7836 - loss: 0.8556 - val_accuracy: 0.8730 - val_loss: 0.4838\n",
            "Epoch 8/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7841 - loss: 0.8859 - val_accuracy: 0.8813 - val_loss: 0.4608\n",
            "Epoch 9/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7982 - loss: 0.8490 - val_accuracy: 0.8709 - val_loss: 0.5570\n",
            "Epoch 10/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7927 - loss: 0.8636 - val_accuracy: 0.8821 - val_loss: 0.5228\n",
            "Epoch 11/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7956 - loss: 0.8831 - val_accuracy: 0.8771 - val_loss: 0.5137\n",
            "Epoch 12/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7972 - loss: 0.8632 - val_accuracy: 0.8786 - val_loss: 0.5115\n",
            "Epoch 13/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7983 - loss: 0.8636 - val_accuracy: 0.8862 - val_loss: 0.5216\n",
            "Epoch 14/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7986 - loss: 0.8541 - val_accuracy: 0.8801 - val_loss: 0.5581\n",
            "Epoch 15/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7982 - loss: 0.8723 - val_accuracy: 0.8796 - val_loss: 0.5935\n",
            "Epoch 16/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7960 - loss: 0.9151 - val_accuracy: 0.8755 - val_loss: 0.5718\n",
            "Epoch 17/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8003 - loss: 0.8871 - val_accuracy: 0.8852 - val_loss: 0.5677\n",
            "Epoch 18/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7939 - loss: 0.8877 - val_accuracy: 0.8736 - val_loss: 0.5736\n",
            "Epoch 19/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8028 - loss: 0.8647 - val_accuracy: 0.8787 - val_loss: 0.6965\n",
            "Epoch 20/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8021 - loss: 0.8690 - val_accuracy: 0.8857 - val_loss: 0.5564\n",
            "Epoch 21/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7994 - loss: 0.8885 - val_accuracy: 0.8861 - val_loss: 0.5908\n",
            "Epoch 22/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7993 - loss: 0.8979 - val_accuracy: 0.8788 - val_loss: 0.5963\n",
            "Epoch 23/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7967 - loss: 0.8927 - val_accuracy: 0.8720 - val_loss: 0.6209\n",
            "Epoch 24/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7934 - loss: 0.9201 - val_accuracy: 0.8826 - val_loss: 0.5632\n",
            "Epoch 25/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8004 - loss: 0.8983 - val_accuracy: 0.8875 - val_loss: 0.5987\n",
            "Epoch 26/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7998 - loss: 0.8993 - val_accuracy: 0.8763 - val_loss: 0.6718\n",
            "Epoch 27/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8019 - loss: 0.8968 - val_accuracy: 0.8796 - val_loss: 0.6293\n",
            "Epoch 28/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8053 - loss: 0.8760 - val_accuracy: 0.8812 - val_loss: 0.6264\n",
            "Epoch 29/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8027 - loss: 0.8865 - val_accuracy: 0.8655 - val_loss: 0.7090\n",
            "Epoch 30/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7999 - loss: 0.9029 - val_accuracy: 0.8822 - val_loss: 0.6009\n",
            "Epoch 31/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8033 - loss: 0.8988 - val_accuracy: 0.8813 - val_loss: 0.6483\n",
            "Epoch 32/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8026 - loss: 0.8765 - val_accuracy: 0.8790 - val_loss: 0.6289\n",
            "Epoch 33/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7993 - loss: 0.9316 - val_accuracy: 0.8801 - val_loss: 0.5917\n",
            "Epoch 34/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7970 - loss: 0.9030 - val_accuracy: 0.8741 - val_loss: 0.6489\n",
            "Epoch 35/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8036 - loss: 0.8798 - val_accuracy: 0.8835 - val_loss: 0.6144\n",
            "Epoch 36/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8019 - loss: 0.8899 - val_accuracy: 0.8790 - val_loss: 0.6473\n",
            "Epoch 37/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8047 - loss: 0.8889 - val_accuracy: 0.8790 - val_loss: 0.6106\n",
            "Epoch 38/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8010 - loss: 0.9243 - val_accuracy: 0.8800 - val_loss: 0.5938\n",
            "Epoch 39/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8004 - loss: 0.8875 - val_accuracy: 0.8772 - val_loss: 0.5871\n",
            "Epoch 40/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7935 - loss: 0.9125 - val_accuracy: 0.8828 - val_loss: 0.6420\n",
            "Epoch 41/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7998 - loss: 0.8976 - val_accuracy: 0.8716 - val_loss: 0.6214\n",
            "Epoch 42/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7987 - loss: 0.9074 - val_accuracy: 0.8814 - val_loss: 0.6373\n",
            "Epoch 43/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8003 - loss: 0.8963 - val_accuracy: 0.8616 - val_loss: 0.5855\n",
            "Epoch 44/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8038 - loss: 0.8561 - val_accuracy: 0.8689 - val_loss: 0.6227\n",
            "Epoch 45/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8041 - loss: 0.8788 - val_accuracy: 0.8779 - val_loss: 0.6616\n",
            "Epoch 46/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - accuracy: 0.8028 - loss: 0.8650 - val_accuracy: 0.8821 - val_loss: 0.7165\n",
            "Epoch 47/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8025 - loss: 0.8817 - val_accuracy: 0.8863 - val_loss: 0.6603\n",
            "Epoch 48/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8094 - loss: 0.8617 - val_accuracy: 0.8814 - val_loss: 0.6092\n",
            "Epoch 49/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7966 - loss: 0.8922 - val_accuracy: 0.8769 - val_loss: 0.6430\n",
            "Epoch 50/50\n",
            "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8000 - loss: 0.8772 - val_accuracy: 0.8810 - val_loss: 0.6263\n",
            "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - accuracy: 0.9050 - loss: 0.5383\n",
            "[INFO] accuracy: 88.10%\n",
            "[INFO] Loss: 0.6263066530227661\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras import applications\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.src.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import utils\n",
        "\n",
        "utils.to_categorical\n",
        "\n",
        "# from tensorflow.keras.\n",
        "import math\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "top_model_weights_path = 'bottleneck_fc_model.weights.h5'\n",
        "train_data_dir = '/Users/saryuvasishat/Desktop/SOEN Project/es-crop-disease-diagnosis-master/dataset/train'\n",
        "validation_data_dir = '/Users/saryuvasishat/Desktop/SOEN Project/es-crop-disease-diagnosis-master/dataset/test'\n",
        "\n",
        "# number of epochs to train top model\n",
        "epochs = 50\n",
        "# batch size used by flow_from_directory and predict_generator\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "def save_bottlebeck_features():\n",
        "    # build the VGG16 network\n",
        "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
        "\n",
        "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "\n",
        "    print(len(generator.filenames))\n",
        "    print(generator.class_indices)\n",
        "    print(len(generator.class_indices))\n",
        "\n",
        "    nb_train_samples = len(generator.filenames)\n",
        "    num_classes = len(generator.class_indices)\n",
        "\n",
        "    predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
        "\n",
        "    bottleneck_features_train = model.predict(\n",
        "        generator, predict_size_train)\n",
        "\n",
        "    np.save('bottleneck_features_train.npy', bottleneck_features_train)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "\n",
        "    nb_validation_samples = len(generator.filenames)\n",
        "\n",
        "    predict_size_validation = int(\n",
        "        math.ceil(nb_validation_samples / batch_size))\n",
        "\n",
        "    bottleneck_features_validation = model.predict_generator(\n",
        "        generator, predict_size_validation)\n",
        "\n",
        "    np.save('bottleneck_features_validation.npy',\n",
        "            bottleneck_features_validation)\n",
        "\n",
        "\n",
        "def save_bottlebeck_features(batch_size):\n",
        "    # build the VGG16 network\n",
        "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
        "\n",
        "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "\n",
        "    print(len(generator.filenames))\n",
        "    print(generator.class_indices)\n",
        "    print(len(generator.class_indices))\n",
        "\n",
        "    nb_train_samples = len(generator.filenames)\n",
        "    num_classes = len(generator.class_indices)\n",
        "\n",
        "    predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
        "\n",
        "    bottleneck_features_train = model.predict(\n",
        "        generator, predict_size_train)\n",
        "\n",
        "    np.save('bottleneck_features_train.npy', bottleneck_features_train)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "\n",
        "    nb_validation_samples = len(generator.filenames)\n",
        "\n",
        "    predict_size_validation = int(\n",
        "        math.ceil(nb_validation_samples / batch_size))\n",
        "\n",
        "    bottleneck_features_validation = model.predict(\n",
        "        generator, predict_size_validation)\n",
        "\n",
        "    np.save('bottleneck_features_validation.npy',\n",
        "            bottleneck_features_validation)\n",
        "\n",
        "\n",
        "def train_top_model(batch_size, epochs):\n",
        "    datagen_top = ImageDataGenerator(rescale=1. / 255)\n",
        "    generator_top = datagen_top.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        "\n",
        "    nb_train_samples = len(generator_top.filenames)\n",
        "    num_classes = len(generator_top.class_indices)\n",
        "\n",
        "    # save the class indices to use use later in predictions\n",
        "    np.save('class_indices.npy', generator_top.class_indices)\n",
        "\n",
        "    train_data = np.load('bottleneck_features_train.npy')\n",
        "\n",
        "    # get the class labels for the training data, in the original order\n",
        "    train_labels = generator_top.classes\n",
        "\n",
        "    train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
        "\n",
        "    generator_top = datagen_top.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "\n",
        "    nb_validation_samples = len(generator_top.filenames)\n",
        "\n",
        "    validation_data = np.load('bottleneck_features_validation.npy')\n",
        "\n",
        "    validation_labels = generator_top.classes\n",
        "    validation_labels = to_categorical(\n",
        "        validation_labels, num_classes=num_classes)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(train_data, train_labels,\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=(validation_data, validation_labels))\n",
        "\n",
        "    model.save_weights(top_model_weights_path)\n",
        "\n",
        "    (eval_loss, eval_accuracy) = model.evaluate(\n",
        "        validation_data, validation_labels, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))\n",
        "    print(\"[INFO] Loss: {}\".format(eval_loss))\n",
        "\n",
        "\n",
        "# batch_size = 16  # Define batch size here\n",
        "# save_bottlebeck_features(batch_size)\n",
        "# train_top_model(batch_size)\n",
        "\n",
        "batch_size = 16  # Define batch size here\n",
        "epochs = 50  # Define number of epochs here\n",
        "\n",
        "save_bottlebeck_features(batch_size)\n",
        "train_top_model(batch_size, epochs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We think for our task, TensorFlow with Keras is better suited than scikit-learn**, as\n",
        "\n",
        "*   The task involves working with CNNs, particularly\n",
        "utilizing the VGG16 architecture. TensorFlow with Keras seamlessly integrates pre-trained CNN models like VGG16, making it easier to implement transfer learning and work with convolutional layers.\n",
        "*   TensorFlow with Keras provides greater flexibility and customization options, allowing for easy customization of network architectures, loss functions, optimizers, and more.\n",
        "\n",
        "Using TensorFlow with Keras provides a robust framework for training learning models to diagnose plant diseases with high accuracy and efficiency."
      ],
      "metadata": {
        "id": "_PwOJ1js9Pax"
      },
      "id": "_PwOJ1js9Pax"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}