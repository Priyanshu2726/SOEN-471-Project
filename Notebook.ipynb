{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7ca9b0-b76b-423f-a23d-37d95ca2b7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function keras.src.utils.numerical_utils.to_categorical(x, num_classes=None)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import applications\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.src.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import utils\n",
    "utils.to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c34136-eeb1-4905-bcca-e1b4ea4a1682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.\n",
    "import math\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "train_data_dir = '/Users/saryuvasishat/Desktop/SOEN Project/es-crop-disease-diagnosis-master/dataset/train'\n",
    "validation_data_dir = '/Users/saryuvasishat/Desktop/SOEN Project/es-crop-disease-diagnosis-master/dataset/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e1496f-a5fe-4f3c-a821-e5ca7cbf38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bottlebeck_features():\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "    print(len(generator.filenames))\n",
    "    print(generator.class_indices)\n",
    "    print(len(generator.class_indices))\n",
    "\n",
    "    nb_train_samples = len(generator.filenames)\n",
    "    num_classes = len(generator.class_indices)\n",
    "\n",
    "    predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
    "\n",
    "    bottleneck_features_train = model.predict(\n",
    "        generator, predict_size_train)\n",
    "\n",
    "    np.save('bottleneck_features_train.npy', bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "    nb_validation_samples = len(generator.filenames)\n",
    "\n",
    "    predict_size_validation = int(\n",
    "        math.ceil(nb_validation_samples / batch_size))\n",
    "\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, predict_size_validation)\n",
    "\n",
    "    np.save('bottleneck_features_validation.npy',\n",
    "            bottleneck_features_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "098abfda-708c-4ee1-b66e-e19e01c09227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43456 images belonging to 38 classes.\n",
      "43456\n",
      "{'Apple___Apple_scab': 0, 'Apple___Black_rot': 1, 'Apple___Cedar_apple_rust': 2, 'Apple___healthy': 3, 'Blueberry___healthy': 4, 'Cherry_(including_sour)___Powdery_mildew': 5, 'Cherry_(including_sour)___healthy': 6, 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': 7, 'Corn_(maize)___Common_rust_': 8, 'Corn_(maize)___Northern_Leaf_Blight': 9, 'Corn_(maize)___healthy': 10, 'Grape___Black_rot': 11, 'Grape___Esca_(Black_Measles)': 12, 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)': 13, 'Grape___healthy': 14, 'Orange___Haunglongbing_(Citrus_greening)': 15, 'Peach___Bacterial_spot': 16, 'Peach___healthy': 17, 'Pepper,_bell___Bacterial_spot': 18, 'Pepper,_bell___healthy': 19, 'Potato___Early_blight': 20, 'Potato___Late_blight': 21, 'Potato___healthy': 22, 'Raspberry___healthy': 23, 'Soybean___healthy': 24, 'Squash___Powdery_mildew': 25, 'Strawberry___Leaf_scorch': 26, 'Strawberry___healthy': 27, 'Tomato___Bacterial_spot': 28, 'Tomato___Early_blight': 29, 'Tomato___Late_blight': 30, 'Tomato___Leaf_Mold': 31, 'Tomato___Septoria_leaf_spot': 32, 'Tomato___Spider_mites Two-spotted_spider_mite': 33, 'Tomato___Target_Spot': 34, 'Tomato___Tomato_Yellow_Leaf_Curl_Virus': 35, 'Tomato___Tomato_mosaic_virus': 36, 'Tomato___healthy': 37}\n",
      "38\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1897s\u001b[0m 698ms/step\n",
      "Found 10849 images belonging to 38 classes.\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m488s\u001b[0m 718ms/step\n",
      "Found 43456 images belonging to 38 classes.\n",
      "Found 10849 images belonging to 38 classes.\n",
      "Epoch 1/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - accuracy: 0.5288 - loss: 1.7366 - val_accuracy: 0.8314 - val_loss: 0.5520\n",
      "Epoch 2/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7317 - loss: 0.9104 - val_accuracy: 0.8468 - val_loss: 0.5003\n",
      "Epoch 3/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7599 - loss: 0.8694 - val_accuracy: 0.8628 - val_loss: 0.4822\n",
      "Epoch 4/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7692 - loss: 0.8509 - val_accuracy: 0.8769 - val_loss: 0.4393\n",
      "Epoch 5/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7753 - loss: 0.8543 - val_accuracy: 0.8799 - val_loss: 0.4469\n",
      "Epoch 6/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7819 - loss: 0.8743 - val_accuracy: 0.8790 - val_loss: 0.4604\n",
      "Epoch 7/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7849 - loss: 0.8801 - val_accuracy: 0.8638 - val_loss: 0.5202\n",
      "Epoch 8/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7850 - loss: 0.8820 - val_accuracy: 0.8854 - val_loss: 0.4842\n",
      "Epoch 9/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7859 - loss: 0.8882 - val_accuracy: 0.8750 - val_loss: 0.5216\n",
      "Epoch 10/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7881 - loss: 0.8801 - val_accuracy: 0.8835 - val_loss: 0.4982\n",
      "Epoch 11/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7889 - loss: 0.8928 - val_accuracy: 0.8779 - val_loss: 0.5201\n",
      "Epoch 12/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7886 - loss: 0.9013 - val_accuracy: 0.8883 - val_loss: 0.5134\n",
      "Epoch 13/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7881 - loss: 0.9015 - val_accuracy: 0.8854 - val_loss: 0.5431\n",
      "Epoch 14/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7917 - loss: 0.8859 - val_accuracy: 0.8737 - val_loss: 0.5458\n",
      "Epoch 15/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7935 - loss: 0.9109 - val_accuracy: 0.8767 - val_loss: 0.5260\n",
      "Epoch 16/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7963 - loss: 0.9123 - val_accuracy: 0.8804 - val_loss: 0.5677\n",
      "Epoch 17/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7954 - loss: 0.9104 - val_accuracy: 0.8848 - val_loss: 0.5653\n",
      "Epoch 18/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7907 - loss: 0.9135 - val_accuracy: 0.8895 - val_loss: 0.5506\n",
      "Epoch 19/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7952 - loss: 0.8855 - val_accuracy: 0.8772 - val_loss: 0.6210\n",
      "Epoch 20/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7968 - loss: 0.8915 - val_accuracy: 0.8772 - val_loss: 0.6074\n",
      "Epoch 21/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7995 - loss: 0.8862 - val_accuracy: 0.8899 - val_loss: 0.5687\n",
      "Epoch 22/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8017 - loss: 0.9060 - val_accuracy: 0.8790 - val_loss: 0.6196\n",
      "Epoch 23/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7984 - loss: 0.9039 - val_accuracy: 0.8828 - val_loss: 0.6223\n",
      "Epoch 24/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8053 - loss: 0.8899 - val_accuracy: 0.8805 - val_loss: 0.6293\n",
      "Epoch 25/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7966 - loss: 0.9197 - val_accuracy: 0.8852 - val_loss: 0.5952\n",
      "Epoch 26/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8025 - loss: 0.8952 - val_accuracy: 0.8843 - val_loss: 0.5996\n",
      "Epoch 27/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8016 - loss: 0.9065 - val_accuracy: 0.8837 - val_loss: 0.6270\n",
      "Epoch 28/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8010 - loss: 0.8819 - val_accuracy: 0.8747 - val_loss: 0.6894\n",
      "Epoch 29/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8032 - loss: 0.8807 - val_accuracy: 0.8748 - val_loss: 0.6281\n",
      "Epoch 30/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8021 - loss: 0.9156 - val_accuracy: 0.8816 - val_loss: 0.6488\n",
      "Epoch 31/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7993 - loss: 0.9109 - val_accuracy: 0.8797 - val_loss: 0.6588\n",
      "Epoch 32/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8011 - loss: 0.8879 - val_accuracy: 0.8763 - val_loss: 0.6850\n",
      "Epoch 33/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8087 - loss: 0.8769 - val_accuracy: 0.8828 - val_loss: 0.6037\n",
      "Epoch 34/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7994 - loss: 0.8983 - val_accuracy: 0.8811 - val_loss: 0.6751\n",
      "Epoch 35/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8048 - loss: 0.8749 - val_accuracy: 0.8697 - val_loss: 0.7084\n",
      "Epoch 36/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8045 - loss: 0.8764 - val_accuracy: 0.8779 - val_loss: 0.7134\n",
      "Epoch 37/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7970 - loss: 0.9040 - val_accuracy: 0.8796 - val_loss: 0.6590\n",
      "Epoch 38/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8014 - loss: 0.8839 - val_accuracy: 0.8591 - val_loss: 0.7273\n",
      "Epoch 39/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7987 - loss: 0.8967 - val_accuracy: 0.8733 - val_loss: 0.7024\n",
      "Epoch 40/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7987 - loss: 0.8866 - val_accuracy: 0.8825 - val_loss: 0.7461\n",
      "Epoch 41/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7983 - loss: 0.8947 - val_accuracy: 0.8803 - val_loss: 0.6808\n",
      "Epoch 42/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8097 - loss: 0.8680 - val_accuracy: 0.8778 - val_loss: 0.7153\n",
      "Epoch 43/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7987 - loss: 0.8981 - val_accuracy: 0.8753 - val_loss: 0.7099\n",
      "Epoch 44/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8031 - loss: 0.8807 - val_accuracy: 0.8800 - val_loss: 0.6964\n",
      "Epoch 45/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8073 - loss: 0.8610 - val_accuracy: 0.8779 - val_loss: 0.6582\n",
      "Epoch 46/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8060 - loss: 0.8738 - val_accuracy: 0.8723 - val_loss: 0.7192\n",
      "Epoch 47/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7978 - loss: 0.9106 - val_accuracy: 0.8660 - val_loss: 0.7586\n",
      "Epoch 48/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8022 - loss: 0.8841 - val_accuracy: 0.8785 - val_loss: 0.7730\n",
      "Epoch 49/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8003 - loss: 0.8946 - val_accuracy: 0.8772 - val_loss: 0.7215\n",
      "Epoch 50/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7984 - loss: 0.9014 - val_accuracy: 0.8810 - val_loss: 0.7284\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The filename must end in `.weights.h5`. Received: filepath=bottleneck_fc_model.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 116\u001b[0m\n\u001b[1;32m    113\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m  \u001b[38;5;66;03m# Define number of epochs here\u001b[39;00m\n\u001b[1;32m    115\u001b[0m save_bottlebeck_features(batch_size)\n\u001b[0;32m--> 116\u001b[0m train_top_model(batch_size, epochs)\n",
      "Cell \u001b[0;32mIn[7], line 98\u001b[0m, in \u001b[0;36mtrain_top_model\u001b[0;34m(batch_size, epochs)\u001b[0m\n\u001b[1;32m     90\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     91\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     93\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_data, train_labels,\n\u001b[1;32m     94\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m     95\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     96\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m(validation_data, validation_labels))\n\u001b[0;32m---> 98\u001b[0m model\u001b[38;5;241m.\u001b[39msave_weights(top_model_weights_path)\n\u001b[1;32m    100\u001b[0m (eval_loss, eval_accuracy) \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m    101\u001b[0m     validation_data, validation_labels, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] accuracy: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(eval_accuracy \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/models/model.py:319\u001b[0m, in \u001b[0;36mModel.save_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Saves all layer weights to a `.weights.h5` file.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m        via an interactive prompt.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe filename must end in `.weights.h5`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     exists \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(filepath)\n",
      "\u001b[0;31mValueError\u001b[0m: The filename must end in `.weights.h5`. Received: filepath=bottleneck_fc_model.h5"
     ]
    }
   ],
   "source": [
    "def save_bottlebeck_features(batch_size):\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "    print(len(generator.filenames))\n",
    "    print(generator.class_indices)\n",
    "    print(len(generator.class_indices))\n",
    "\n",
    "    nb_train_samples = len(generator.filenames)\n",
    "    num_classes = len(generator.class_indices)\n",
    "\n",
    "    predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
    "\n",
    "    bottleneck_features_train = model.predict(\n",
    "        generator, predict_size_train)\n",
    "\n",
    "    np.save('bottleneck_features_train.npy', bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "    nb_validation_samples = len(generator.filenames)\n",
    "\n",
    "    predict_size_validation = int(\n",
    "        math.ceil(nb_validation_samples / batch_size))\n",
    "\n",
    "    bottleneck_features_validation = model.predict(\n",
    "        generator, predict_size_validation)\n",
    "\n",
    "    np.save('bottleneck_features_validation.npy',\n",
    "            bottleneck_features_validation)\n",
    "\n",
    "def train_top_model(batch_size, epochs):\n",
    "    datagen_top = ImageDataGenerator(rescale=1. / 255)\n",
    "    generator_top = datagen_top.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "    nb_train_samples = len(generator_top.filenames)\n",
    "    num_classes = len(generator_top.class_indices)\n",
    "\n",
    "    # save the class indices to use use later in predictions\n",
    "    np.save('class_indices.npy', generator_top.class_indices)\n",
    "\n",
    "    # load the bottleneck features saved earlier\n",
    "    train_data = np.load('bottleneck_features_train.npy')\n",
    "\n",
    "    # get the class labels for the training data, in the original order\n",
    "    train_labels = generator_top.classes\n",
    "\n",
    "    train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "\n",
    "    generator_top = datagen_top.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "    nb_validation_samples = len(generator_top.filenames)\n",
    "\n",
    "    validation_data = np.load('bottleneck_features_validation.npy')\n",
    "\n",
    "    validation_labels = generator_top.classes\n",
    "    validation_labels = to_categorical(\n",
    "        validation_labels, num_classes=num_classes)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_data, train_labels,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(validation_data, validation_labels))\n",
    "\n",
    "    model.save_weights(top_model_weights_path)\n",
    "\n",
    "    (eval_loss, eval_accuracy) = model.evaluate(\n",
    "        validation_data, validation_labels, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))\n",
    "    print(\"[INFO] Loss: {}\".format(eval_loss))\n",
    "\n",
    "\n",
    "\n",
    "#batch_size = 16  # Define batch size here\n",
    "#save_bottlebeck_features(batch_size)\n",
    "#train_top_model(batch_size)\n",
    "\n",
    "batch_size = 16  # Define batch size here\n",
    "epochs = 50  # Define number of epochs here\n",
    "\n",
    "save_bottlebeck_features(batch_size)\n",
    "train_top_model(batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "104a88aa-e841-4269-a664-2d77609066cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43456 images belonging to 38 classes.\n",
      "43456\n",
      "{'Apple___Apple_scab': 0, 'Apple___Black_rot': 1, 'Apple___Cedar_apple_rust': 2, 'Apple___healthy': 3, 'Blueberry___healthy': 4, 'Cherry_(including_sour)___Powdery_mildew': 5, 'Cherry_(including_sour)___healthy': 6, 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': 7, 'Corn_(maize)___Common_rust_': 8, 'Corn_(maize)___Northern_Leaf_Blight': 9, 'Corn_(maize)___healthy': 10, 'Grape___Black_rot': 11, 'Grape___Esca_(Black_Measles)': 12, 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)': 13, 'Grape___healthy': 14, 'Orange___Haunglongbing_(Citrus_greening)': 15, 'Peach___Bacterial_spot': 16, 'Peach___healthy': 17, 'Pepper,_bell___Bacterial_spot': 18, 'Pepper,_bell___healthy': 19, 'Potato___Early_blight': 20, 'Potato___Late_blight': 21, 'Potato___healthy': 22, 'Raspberry___healthy': 23, 'Soybean___healthy': 24, 'Squash___Powdery_mildew': 25, 'Strawberry___Leaf_scorch': 26, 'Strawberry___healthy': 27, 'Tomato___Bacterial_spot': 28, 'Tomato___Early_blight': 29, 'Tomato___Late_blight': 30, 'Tomato___Leaf_Mold': 31, 'Tomato___Septoria_leaf_spot': 32, 'Tomato___Spider_mites Two-spotted_spider_mite': 33, 'Tomato___Target_Spot': 34, 'Tomato___Tomato_Yellow_Leaf_Curl_Virus': 35, 'Tomato___Tomato_mosaic_virus': 36, 'Tomato___healthy': 37}\n",
      "38\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1902s\u001b[0m 700ms/step\n",
      "Found 10849 images belonging to 38 classes.\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 690ms/step\n",
      "Found 43456 images belonging to 38 classes.\n",
      "Found 10849 images belonging to 38 classes.\n",
      "Epoch 1/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.5436 - loss: 1.6807 - val_accuracy: 0.8468 - val_loss: 0.4980\n",
      "Epoch 2/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7376 - loss: 0.9049 - val_accuracy: 0.8581 - val_loss: 0.4665\n",
      "Epoch 3/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7595 - loss: 0.8668 - val_accuracy: 0.8652 - val_loss: 0.4716\n",
      "Epoch 4/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7718 - loss: 0.8433 - val_accuracy: 0.8632 - val_loss: 0.4881\n",
      "Epoch 5/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7793 - loss: 0.8563 - val_accuracy: 0.8746 - val_loss: 0.4934\n",
      "Epoch 6/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7805 - loss: 0.8801 - val_accuracy: 0.8804 - val_loss: 0.4709\n",
      "Epoch 7/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7836 - loss: 0.8556 - val_accuracy: 0.8730 - val_loss: 0.4838\n",
      "Epoch 8/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7841 - loss: 0.8859 - val_accuracy: 0.8813 - val_loss: 0.4608\n",
      "Epoch 9/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7982 - loss: 0.8490 - val_accuracy: 0.8709 - val_loss: 0.5570\n",
      "Epoch 10/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7927 - loss: 0.8636 - val_accuracy: 0.8821 - val_loss: 0.5228\n",
      "Epoch 11/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7956 - loss: 0.8831 - val_accuracy: 0.8771 - val_loss: 0.5137\n",
      "Epoch 12/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7972 - loss: 0.8632 - val_accuracy: 0.8786 - val_loss: 0.5115\n",
      "Epoch 13/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7983 - loss: 0.8636 - val_accuracy: 0.8862 - val_loss: 0.5216\n",
      "Epoch 14/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7986 - loss: 0.8541 - val_accuracy: 0.8801 - val_loss: 0.5581\n",
      "Epoch 15/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7982 - loss: 0.8723 - val_accuracy: 0.8796 - val_loss: 0.5935\n",
      "Epoch 16/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7960 - loss: 0.9151 - val_accuracy: 0.8755 - val_loss: 0.5718\n",
      "Epoch 17/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8003 - loss: 0.8871 - val_accuracy: 0.8852 - val_loss: 0.5677\n",
      "Epoch 18/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7939 - loss: 0.8877 - val_accuracy: 0.8736 - val_loss: 0.5736\n",
      "Epoch 19/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8028 - loss: 0.8647 - val_accuracy: 0.8787 - val_loss: 0.6965\n",
      "Epoch 20/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8021 - loss: 0.8690 - val_accuracy: 0.8857 - val_loss: 0.5564\n",
      "Epoch 21/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7994 - loss: 0.8885 - val_accuracy: 0.8861 - val_loss: 0.5908\n",
      "Epoch 22/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7993 - loss: 0.8979 - val_accuracy: 0.8788 - val_loss: 0.5963\n",
      "Epoch 23/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7967 - loss: 0.8927 - val_accuracy: 0.8720 - val_loss: 0.6209\n",
      "Epoch 24/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7934 - loss: 0.9201 - val_accuracy: 0.8826 - val_loss: 0.5632\n",
      "Epoch 25/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8004 - loss: 0.8983 - val_accuracy: 0.8875 - val_loss: 0.5987\n",
      "Epoch 26/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7998 - loss: 0.8993 - val_accuracy: 0.8763 - val_loss: 0.6718\n",
      "Epoch 27/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8019 - loss: 0.8968 - val_accuracy: 0.8796 - val_loss: 0.6293\n",
      "Epoch 28/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8053 - loss: 0.8760 - val_accuracy: 0.8812 - val_loss: 0.6264\n",
      "Epoch 29/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8027 - loss: 0.8865 - val_accuracy: 0.8655 - val_loss: 0.7090\n",
      "Epoch 30/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7999 - loss: 0.9029 - val_accuracy: 0.8822 - val_loss: 0.6009\n",
      "Epoch 31/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8033 - loss: 0.8988 - val_accuracy: 0.8813 - val_loss: 0.6483\n",
      "Epoch 32/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8026 - loss: 0.8765 - val_accuracy: 0.8790 - val_loss: 0.6289\n",
      "Epoch 33/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7993 - loss: 0.9316 - val_accuracy: 0.8801 - val_loss: 0.5917\n",
      "Epoch 34/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7970 - loss: 0.9030 - val_accuracy: 0.8741 - val_loss: 0.6489\n",
      "Epoch 35/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8036 - loss: 0.8798 - val_accuracy: 0.8835 - val_loss: 0.6144\n",
      "Epoch 36/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8019 - loss: 0.8899 - val_accuracy: 0.8790 - val_loss: 0.6473\n",
      "Epoch 37/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8047 - loss: 0.8889 - val_accuracy: 0.8790 - val_loss: 0.6106\n",
      "Epoch 38/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8010 - loss: 0.9243 - val_accuracy: 0.8800 - val_loss: 0.5938\n",
      "Epoch 39/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8004 - loss: 0.8875 - val_accuracy: 0.8772 - val_loss: 0.5871\n",
      "Epoch 40/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7935 - loss: 0.9125 - val_accuracy: 0.8828 - val_loss: 0.6420\n",
      "Epoch 41/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7998 - loss: 0.8976 - val_accuracy: 0.8716 - val_loss: 0.6214\n",
      "Epoch 42/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7987 - loss: 0.9074 - val_accuracy: 0.8814 - val_loss: 0.6373\n",
      "Epoch 43/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8003 - loss: 0.8963 - val_accuracy: 0.8616 - val_loss: 0.5855\n",
      "Epoch 44/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8038 - loss: 0.8561 - val_accuracy: 0.8689 - val_loss: 0.6227\n",
      "Epoch 45/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8041 - loss: 0.8788 - val_accuracy: 0.8779 - val_loss: 0.6616\n",
      "Epoch 46/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - accuracy: 0.8028 - loss: 0.8650 - val_accuracy: 0.8821 - val_loss: 0.7165\n",
      "Epoch 47/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8025 - loss: 0.8817 - val_accuracy: 0.8863 - val_loss: 0.6603\n",
      "Epoch 48/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8094 - loss: 0.8617 - val_accuracy: 0.8814 - val_loss: 0.6092\n",
      "Epoch 49/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7966 - loss: 0.8922 - val_accuracy: 0.8769 - val_loss: 0.6430\n",
      "Epoch 50/50\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8000 - loss: 0.8772 - val_accuracy: 0.8810 - val_loss: 0.6263\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - accuracy: 0.9050 - loss: 0.5383\n",
      "[INFO] accuracy: 88.10%\n",
      "[INFO] Loss: 0.6263066530227661\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import applications\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.src.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "utils.to_categorical\n",
    "\n",
    "# from tensorflow.keras.\n",
    "import math\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.weights.h5'\n",
    "train_data_dir = '/Users/saryuvasishat/Desktop/SOEN Project/es-crop-disease-diagnosis-master/dataset/train'\n",
    "validation_data_dir = '/Users/saryuvasishat/Desktop/SOEN Project/es-crop-disease-diagnosis-master/dataset/test'\n",
    "\n",
    "# number of epochs to train top model\n",
    "epochs = 50\n",
    "# batch size used by flow_from_directory and predict_generator\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "def save_bottlebeck_features():\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "    print(len(generator.filenames))\n",
    "    print(generator.class_indices)\n",
    "    print(len(generator.class_indices))\n",
    "\n",
    "    nb_train_samples = len(generator.filenames)\n",
    "    num_classes = len(generator.class_indices)\n",
    "\n",
    "    predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
    "\n",
    "    bottleneck_features_train = model.predict(\n",
    "        generator, predict_size_train)\n",
    "\n",
    "    np.save('bottleneck_features_train.npy', bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "    nb_validation_samples = len(generator.filenames)\n",
    "\n",
    "    predict_size_validation = int(\n",
    "        math.ceil(nb_validation_samples / batch_size))\n",
    "\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, predict_size_validation)\n",
    "\n",
    "    np.save('bottleneck_features_validation.npy',\n",
    "            bottleneck_features_validation)\n",
    "\n",
    "\n",
    "def save_bottlebeck_features(batch_size):\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "    print(len(generator.filenames))\n",
    "    print(generator.class_indices)\n",
    "    print(len(generator.class_indices))\n",
    "\n",
    "    nb_train_samples = len(generator.filenames)\n",
    "    num_classes = len(generator.class_indices)\n",
    "\n",
    "    predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
    "\n",
    "    bottleneck_features_train = model.predict(\n",
    "        generator, predict_size_train)\n",
    "\n",
    "    np.save('bottleneck_features_train.npy', bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "    nb_validation_samples = len(generator.filenames)\n",
    "\n",
    "    predict_size_validation = int(\n",
    "        math.ceil(nb_validation_samples / batch_size))\n",
    "\n",
    "    bottleneck_features_validation = model.predict(\n",
    "        generator, predict_size_validation)\n",
    "\n",
    "    np.save('bottleneck_features_validation.npy',\n",
    "            bottleneck_features_validation)\n",
    "\n",
    "\n",
    "def train_top_model(batch_size, epochs):\n",
    "    datagen_top = ImageDataGenerator(rescale=1. / 255)\n",
    "    generator_top = datagen_top.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "    nb_train_samples = len(generator_top.filenames)\n",
    "    num_classes = len(generator_top.class_indices)\n",
    "\n",
    "    # save the class indices to use use later in predictions\n",
    "    np.save('class_indices.npy', generator_top.class_indices)\n",
    "\n",
    "    train_data = np.load('bottleneck_features_train.npy')\n",
    "\n",
    "    # get the class labels for the training data, in the original order\n",
    "    train_labels = generator_top.classes\n",
    "\n",
    "    train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "\n",
    "    generator_top = datagen_top.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "    nb_validation_samples = len(generator_top.filenames)\n",
    "\n",
    "    validation_data = np.load('bottleneck_features_validation.npy')\n",
    "\n",
    "    validation_labels = generator_top.classes\n",
    "    validation_labels = to_categorical(\n",
    "        validation_labels, num_classes=num_classes)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_data, train_labels,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(validation_data, validation_labels))\n",
    "\n",
    "    model.save_weights(top_model_weights_path)\n",
    "\n",
    "    (eval_loss, eval_accuracy) = model.evaluate(\n",
    "        validation_data, validation_labels, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))\n",
    "    print(\"[INFO] Loss: {}\".format(eval_loss))\n",
    "\n",
    "\n",
    "# batch_size = 16  # Define batch size here\n",
    "# save_bottlebeck_features(batch_size)\n",
    "# train_top_model(batch_size)\n",
    "\n",
    "batch_size = 16  # Define batch size here\n",
    "epochs = 50  # Define number of epochs here\n",
    "\n",
    "save_bottlebeck_features(batch_size)\n",
    "train_top_model(batch_size, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab42a2e-39c4-4af7-8ece-4dc0a7de250a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
